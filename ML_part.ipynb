{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "This contains the functions that i use to prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries for data manipulation, machine learning, and visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from nltk.stem import PorterStemmer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold, cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all necessary NLTK resources are available\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup stopwords and stemming \n",
    "default_stopwords = set(stopwords.words('english')) | {\n",
    "    'said', 'would', 'even', 'according', 'could', 'year', 'years', 'also', 'new', 'people', \n",
    "    'old', 'one', 'two', 'time', 'first', 'last', 'say', 'make', 'best', 'get', 'three', \n",
    "    'year old', 'told', 'made', 'like', 'take', 'many', 'set', 'number', 'month', 'week', \n",
    "    'well', 'back'\n",
    "}\n",
    "\n",
    "#Stemming \n",
    "stemmer = PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Clean text by lowercasing, removing stopwords, digits, and non-alphabetic characters,\n",
    "    and applying stemming.\"\"\"\n",
    "\n",
    "    # Normalize text: lowercasing and removing non-alphabetic characters\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text.lower())\n",
    "\n",
    "    # Tokenization and filtering out stopwords and short words\n",
    "    tokens = [word for word in word_tokenize(text) if word not in default_stopwords and len(word) > 3]\n",
    "\n",
    "    # Stemming\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "    # Join cleaned tokens back into a string\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(file_path):\n",
    "    \"\"\"Load and preprocess dataset.\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['text'] = df['title'] + ' ' + df['content']  # Combine title and content\n",
    "    df['text'] = df['text'].apply(clean_text)  # Clean text\n",
    "\n",
    "    # Keep only the 'text' column as a feature \n",
    "    df = df[['text', 'category_level_1', 'category_level_2']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_vectorize(texts):\n",
    "    \"\"\"Vectorize text using TF-IDF with optimized settings. I try different parameters \"\"\"\n",
    "    vectorizer = TfidfVectorizer(max_features=30000,  # I should reduce if overfitting\n",
    "                                 ngram_range=(1, 3),  # Try using trigrams\n",
    "                                 min_df=3,  # Minimum document frequency\n",
    "                                 max_df=0.7)  # Max proportion of docs containing the term\n",
    "    \n",
    "    #different parameters using \n",
    "    #vectorizer =TfidfVectorizer(strip_accents='unicode', analyzer='word', ngram_range=(1,2), norm='l2', max_features=50000)\n",
    "    return vectorizer.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot metrics Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score,f1_score\n",
    "\n",
    "\n",
    "# Function to plot confusion matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, labels, title='Confusion Matrix'):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "#not really helpful, quite unecessary \n",
    "def print_qualitative_results(model, X_test, y_test, labels, num_examples=5):\n",
    "    y_pred = model.predict(X_test)\n",
    "    for i in range(num_examples):\n",
    "        print(f\"Input Text: {X_test[i]}\")\n",
    "        print(f\"Predicted: {labels[y_pred[i]]}\")\n",
    "        print(f\"Ground Truth: {labels[y_test[i]]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cross_validation_models(model, X, y):\n",
    "    \n",
    "    # Setting up cross-validation with KFold \n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Performing cross-validation to get predictions\n",
    "    y_pred = cross_val_predict(model, X, y, cv=kf)\n",
    "    \n",
    "    # Computing the confusion matrix\n",
    "    conf_matrix = confusion_matrix(y, y_pred)\n",
    "    \n",
    "    # Plotting the confusion matrix\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues', cbar=False)\n",
    "    plt.xlabel('Predicted labels')\n",
    "    plt.ylabel('True labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculating other metrics from confusion matrix\n",
    "    accuracy = np.trace(conf_matrix) / float(np.sum(conf_matrix))\n",
    "    precision = np.diag(conf_matrix) / np.sum(conf_matrix, axis=0)\n",
    "    recall = np.diag(conf_matrix) / np.sum(conf_matrix, axis=1)\n",
    "    f1_score = 2 * precision * recall / (precision + recall)\n",
    "    \n",
    "    # the results for each metric into mean scores to summarize the performance\n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': np.nanmean(precision),  # using nanmean to avoid NaN values affecting the mean calculation\n",
    "        'recall': np.nanmean(recall),\n",
    "        'f1_score': np.nanmean(f1_score)\n",
    "    }\n",
    "    \n",
    "    # Outputting the results for quick inspection\n",
    "    print(f\"Cross-Validated Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"Cross-Validated Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"Cross-Validated Recall: {metrics['recall']:.4f}\")\n",
    "    print(f\"Cross-Validated F1-Score: {metrics['f1_score']:.4f}\")\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load and preprocess data\n",
    "file_path = '/Users/theodorapoultsidou/Desktop/machinelearning/news-classification.csv'\n",
    "df = load_and_preprocess_data(file_path)\n",
    "\n",
    "# Vectorize text data\n",
    "tfidf_vectorized_data = tfidf_vectorize(df['text'])\n",
    "\n",
    "# Encode labels for Level 1 and Level 2 categories\n",
    "label_encoder_1 = LabelEncoder()\n",
    "y_level_1 = label_encoder_1.fit_transform(df['category_level_1'])\n",
    "\n",
    "label_encoder_2 = LabelEncoder()\n",
    "y_level_2 = label_encoder_2.fit_transform(df['category_level_2'])\n",
    "\n",
    "# Split the data for level 1\n",
    "X_train, X_test, y_train_level_1, y_test_level_1 = train_test_split(tfidf_vectorized_data, y_level_1, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the data for level 2\n",
    "X_train_level_2, X_test_level_2, y_train_level_2, y_test_level_2 = train_test_split(tfidf_vectorized_data, y_level_2, test_size=0.2, random_state=42)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information about the dataset\n",
    "print(\"Basic Information:\")\n",
    "print(df.info())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the distribution of the target variables\n",
    "print(\"\\nDistribution of Category Level 1:\")\n",
    "print(df['category_level_1'].value_counts())\n",
    "\n",
    "print(\"\\nDistribution of Category Level 2:\")\n",
    "print(df['category_level_2'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test different alpha values for Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the range of alpha values to test\n",
    "alpha_values = np.logspace(-3, 1, 10)  # 10 values from 0.001 to 10\n",
    "\n",
    "# Lists to store the results\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "# Evaluate the model for each alpha value\n",
    "for alpha in alpha_values:\n",
    "    model = MultinomialNB(alpha=alpha)\n",
    "    ovr_model = OneVsRestClassifier(model)\n",
    "    \n",
    "    # Cross-validation scores\n",
    "    metrics_nb_l1 = cross_validation_models(ovr_model, tfidf_vectorized_data, y_level_1)\n",
    "    test_accuracy = np.mean(metrics_nb_l1['accuracy'])\n",
    "    \n",
    "    # Fit the model on the entire training set and evaluate on the test set\n",
    "    ovr_model.fit(X_train, y_train_level_1)\n",
    "    train_accuracy = ovr_model.score(X_train, y_train_level_1)\n",
    "    test_accuracy = ovr_model.score(X_test, y_test_level_1)\n",
    "    \n",
    "    train_accuracies.append(train_accuracy)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "\n",
    "# Plot the accuracy curve\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.plot(alpha_values, train_accuracies, label='Train Accuracy', marker='o')\n",
    "plt.plot(alpha_values, test_accuracies, label='Test Accuracy', marker='o')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Naive Bayes Accuracy vs. Alpha')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and cross evaluate models for category_level_1 \n",
    "model_nb_l1  = MultinomialNB(alpha=0.1)\n",
    "metrics_nb_l1 = cross_validation_models(model_nb_l1, tfidf_vectorized_data, y_level_1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test different k for the KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train_level_1, y_test_level_1 = train_test_split(tfidf_vectorized_data, y_level_1, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the range of k values to test with a step of 2\n",
    "k_values = range(1, 21, 4)  # k from 1 to 20 with step of 4\n",
    "\n",
    "# Lists to store the results\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "# Evaluate the model for each k value\n",
    "for k in k_values:\n",
    "    model = KNeighborsClassifier(n_neighbors=k)\n",
    "    \n",
    "    # Fit the model on the entire training set and evaluate on the test set\n",
    "    model.fit(X_train, y_train_level_1)\n",
    "    train_accuracy = model.score(X_train, y_train_level_1)\n",
    "    test_accuracy = model.score(X_test, y_test_level_1)\n",
    "    \n",
    "    train_accuracies.append(train_accuracy)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "\n",
    "# Plot the accuracy curve\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(k_values, train_accuracies, label='Train Accuracy', marker='o')\n",
    "plt.plot(k_values, test_accuracies, label='Test Accuracy', marker='o')\n",
    "plt.xlabel('Number of Neighbors (k)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('KNN Accuracy vs. Number of Neighbors (k)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and cross evaluate models for category_level_1 with KNN\n",
    "model_knn_l1 = KNeighborsClassifier(n_neighbors=16)\n",
    "metrics_knn_l1 = cross_validation_models(model_knn_l1, tfidf_vectorized_data, y_level_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### try different C for the logistic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the range of C values to test\n",
    "C_values = np.logspace(-3, 2, 10)  # 10 values from 0.001 to 1000\n",
    "\n",
    "# Lists to store the results\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "# Evaluate the model for each C value\n",
    "for C in C_values:\n",
    "    model = LogisticRegression(C=C, solver='sag', max_iter=1000)\n",
    "    ovr_model = OneVsRestClassifier(model)\n",
    "    \n",
    "    # Cross-validation scores\n",
    "    metrics_lr_l1 = cross_validation_models(ovr_model, tfidf_vectorized_data, y_level_1)\n",
    "    test_accuracy = np.mean(metrics_lr_l1['accuracy'])\n",
    "    \n",
    "    # Fit the model on the entire training set and evaluate on the test set\n",
    "    ovr_model.fit(X_train, y_train_level_1)\n",
    "    train_accuracy = ovr_model.score(X_train, y_train_level_1)\n",
    "    test_accuracy = ovr_model.score(X_test, y_test_level_1)\n",
    "    \n",
    "    train_accuracies.append(train_accuracy)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "\n",
    "# Plot the accuracy curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(C_values, train_accuracies, label='Train Accuracy', marker='o')\n",
    "plt.plot(C_values, test_accuracies, label='Test Accuracy', marker='o')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Regularization Strength (C)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Logistic Regression Accuracy vs. Regularization Strength (C)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and cross evaluate models for category_level_1 with Logistic Regression\n",
    "model_lr_l1 = OneVsRestClassifier(LogisticRegression(C=100,solver='sag', max_iter=1000))\n",
    "metrics_lr_l1 = cross_validation_models(model_lr_l1, tfidf_vectorized_data, y_level_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### try different C for the SVC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train_level_1, y_test_level_1 = train_test_split(tfidf_vectorized_data, y_level_1, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Define the range of C values to test\n",
    "C_values = np.logspace(-5, 15, num=11, base=2) \n",
    "# Lists to store the results\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "# Evaluate the model for each C value\n",
    "for C in C_values:\n",
    "    model = SVC(C=C, kernel='linear')\n",
    "    ovr_model = OneVsRestClassifier(model)\n",
    "    \n",
    "    # Fit the model on the entire training set and evaluate on the test set\n",
    "    ovr_model.fit(X_train, y_train_level_1)\n",
    "    train_accuracy = ovr_model.score(X_train, y_train_level_1)\n",
    "    test_accuracy = ovr_model.score(X_test, y_test_level_1)\n",
    "    \n",
    "    train_accuracies.append(train_accuracy)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "\n",
    "# Plot the accuracy curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(C_values, train_accuracies, label='Train Accuracy', marker='o')\n",
    "plt.plot(C_values, test_accuracies, label='Test Accuracy', marker='o')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Regularization Parameter (C)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('SVC Accuracy vs. Regularization Parameter (C)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC\n",
    "model_svc_l1 = OneVsRestClassifier(SVC(C=10,kernel='linear'))\n",
    "metrics_svc_l1 = cross_validation_models(model_svc_l1, tfidf_vectorized_data, y_level_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "model_knn_l2 = KNeighborsClassifier(n_neighbors=16)\n",
    "metrics_knn_l2 = cross_validation_models(model_knn_l2, X_train_level_2, y_train_level_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and cross evaluate models for category_level_2\n",
    "model_nb_l2 = MultinomialNB(alpha=0.1)\n",
    "metrics_nb_l2 = cross_validation_models(model_nb_l2, tfidf_vectorized_data, y_level_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "model_lr_l2 = OneVsRestClassifier(LogisticRegression(C=100,solver='sag',max_iter=1000))\n",
    "metrics_lr_l2 = cross_validation_models(model_lr_l2, tfidf_vectorized_data, y_level_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC\n",
    "model_svc_l2 = OneVsRestClassifier(SVC(C=10, kernel='linear'))\n",
    "metrics_svc_l2 = cross_validation_models(model_svc_l2, tfidf_vectorized_data, y_level_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# Initialize a dictionary to store results for each Level 1 category\n",
    "hierarchical_results = {}\n",
    "model_aggregate_metrics = {\n",
    "    'Naive Bayes': {'Accuracy': [], 'Precision': [], 'Recall': [], 'F1-Score': [], 'Training Time': [], 'Prediction Time': []},\n",
    "    'Logistic Regression': {'Accuracy': [], 'Precision': [], 'Recall': [], 'F1-Score': [], 'Training Time': [], 'Prediction Time': []},\n",
    "    'SVC': {'Accuracy': [], 'Precision': [], 'Recall': [], 'F1-Score': [], 'Training Time': [], 'Prediction Time': []},\n",
    "    'KNN': {'Accuracy': [], 'Precision': [], 'Recall': [], 'F1-Score': [], 'Training Time': [], 'Prediction Time': []}\n",
    "}\n",
    "\n",
    "def measure_time(model, X_train, X_test, y_train, y_test):\n",
    "    start_train = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    end_train = time.time()\n",
    "    \n",
    "    start_predict = time.time()\n",
    "    y_pred = model.predict(X_test)\n",
    "    end_predict = time.time()\n",
    "    \n",
    "    training_time = end_train - start_train\n",
    "    prediction_time = end_predict - start_predict\n",
    "    \n",
    "    return training_time, prediction_time\n",
    "\n",
    "for category in label_encoder_1.classes_:\n",
    "    print(f\"Processing category: {category}\")\n",
    "    df_category = df[df['category_level_1'] == category]\n",
    "    \n",
    "    if df_category.empty:\n",
    "        print(f\"No data available for category {category}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    X_category = tfidf_vectorizer.fit_transform(df_category['text'])\n",
    "    y_category = label_encoder_2.transform(df_category['category_level_2'])\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_category, y_category, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train and evaluate models, storing and printing results\n",
    "    for model_name, model in {\n",
    "        'Naive Bayes': OneVsRestClassifier(MultinomialNB(alpha=0.1)),\n",
    "        'Logistic Regression':  OneVsRestClassifier(LogisticRegression(C=100,solver='sag',max_iter=1000 )),\n",
    "        'SVC':  OneVsRestClassifier(SVC(C=10, kernel='linear')),\n",
    "        'KNN': KNeighborsClassifier(n_neighbors=6)\n",
    "    }.items():\n",
    "        train_time, predict_time = measure_time(model, X_train, X_test, y_train, y_test)\n",
    "        predictions = model.predict(X_test)\n",
    "        \n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "        precision = precision_score(y_test, predictions, average='macro')\n",
    "        recall = recall_score(y_test, predictions, average='macro')\n",
    "        f1 = f1_score(y_test, predictions, average='macro')\n",
    "        \n",
    "        model_aggregate_metrics[model_name]['Accuracy'].append(accuracy)\n",
    "        model_aggregate_metrics[model_name]['Precision'].append(precision)\n",
    "        model_aggregate_metrics[model_name]['Recall'].append(recall)\n",
    "        model_aggregate_metrics[model_name]['F1-Score'].append(f1)\n",
    "        model_aggregate_metrics[model_name]['Training Time'].append(train_time)\n",
    "        model_aggregate_metrics[model_name]['Prediction Time'].append(predict_time)\n",
    "\n",
    "        hierarchical_results.setdefault(category, {})[model_name] = {\n",
    "            'Accuracy': accuracy,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1-Score': f1,\n",
    "            'Training Time': train_time,\n",
    "            'Prediction Time': predict_time\n",
    "        }\n",
    "        print(f\"Results for {category} - {model_name}: {hierarchical_results[category][model_name]}\")\n",
    "\n",
    "# Calculate and print average metrics for each model across all categories\n",
    "for model_name, metrics in model_aggregate_metrics.items():\n",
    "    print(f\"\\nAverage Metrics for {model_name}:\")\n",
    "    for metric_name, values in metrics.items():\n",
    "        average_metric = np.mean(values)\n",
    "        print(f\"{metric_name}: {average_metric:.4f}\")\n",
    "\n",
    "# Print the average training and prediction times for each model\n",
    "print(\"\\nAverage Training and Prediction Times for Each Model:\")\n",
    "for model_name, metrics in model_aggregate_metrics.items():\n",
    "    avg_training_time = np.mean(metrics['Training Time'])\n",
    "    avg_prediction_time = np.mean(metrics['Prediction Time'])\n",
    "    print(f\"{model_name}:\")\n",
    "    print(f\"  Average Training Time: {avg_training_time:.4f} seconds\")\n",
    "    print(f\"  Average Prediction Time: {avg_prediction_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print average metrics for each model across all categories\n",
    "for model_name, metrics in model_aggregate_metrics.items():\n",
    "    print(f\"\\nAverage Metrics for {model_name}:\")\n",
    "    for metric_name, values in metrics.items():\n",
    "        average_metric = np.mean(values)\n",
    "        print(f\"{metric_name}: {average_metric:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the time complexity, I run it on a single split, not kfold cross validation, to test it with the hierarchical which is on a single split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_vectorized_data, y_level_1, test_size=0.2, random_state=42)\n",
    "\n",
    "nb = (MultinomialNB(alpha=0.1))\n",
    "lr =  OneVsRestClassifier(LogisticRegression(C=100,solver='sag',max_iter=1000 ))\n",
    "svc = OneVsRestClassifier(SVC(C=10, kernel='linear'))\n",
    "knn = KNeighborsClassifier(n_neighbors=10)\n",
    "\n",
    "\n",
    "def measure_time(model, X_train, X_test, y_train, y_test):\n",
    "    start_train = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    end_train = time.time()\n",
    "    \n",
    "    start_predict = time.time()\n",
    "    y_pred = model.predict(X_test)\n",
    "    end_predict = time.time()\n",
    "    \n",
    "    training_time = end_train - start_train\n",
    "    prediction_time = end_predict - start_predict\n",
    "    \n",
    "    return training_time, prediction_time\n",
    "\n",
    "nb_train_time, nb_predict_time = measure_time(nb, X_train, X_test, y_train, y_test)\n",
    "lr_train_time, lr_predict_time = measure_time(lr, X_train, X_test, y_train, y_test)\n",
    "svc_train_time, svc_predict_time = measure_time(svc, X_train, X_test, y_train, y_test)\n",
    "knn_train_time, knn_predict_time = measure_time(knn, X_train, X_test, y_train, y_test)\n",
    "\n",
    "print(f\"Naive Bayes - Training Time: {nb_train_time:.4f}s, Prediction Time: {nb_predict_time:.4f}s\")\n",
    "print(f\"Logistic Regression - Training Time: {lr_train_time:.4f}s, Prediction Time: {lr_predict_time:.4f}s\")\n",
    "print(f\"SVC - Training Time: {svc_train_time:.4f}s, Prediction Time: {svc_predict_time:.4f}s\")\n",
    "print(f\"KNN - Training Time: {knn_train_time:.4f}s, Prediction Time: {knn_predict_time:.4f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_level_2, X_test_level_2, y_train, y_test = train_test_split(tfidf_vectorized_data, y_level_2, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "nb = (MultinomialNB(alpha=0.1))\n",
    "lr =  OneVsRestClassifier(LogisticRegression(C=100,solver='sag',max_iter=1000 ))\n",
    "svc = OneVsRestClassifier(SVC(C=10, kernel='linear'))\n",
    "knn = KNeighborsClassifier(n_neighbors=10)\n",
    "\n",
    "\n",
    "def measure_time(model, X_train, X_test, y_train, y_test):\n",
    "    start_train = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    end_train = time.time()\n",
    "    \n",
    "    start_predict = time.time()\n",
    "    y_pred = model.predict(X_test)\n",
    "    end_predict = time.time()\n",
    "    \n",
    "    training_time = end_train - start_train\n",
    "    prediction_time = end_predict - start_predict\n",
    "    \n",
    "    return training_time, prediction_time\n",
    "\n",
    "nb_train_time, nb_predict_time = measure_time(nb, X_train, X_test, y_train, y_test)\n",
    "lr_train_time, lr_predict_time = measure_time(lr, X_train, X_test, y_train, y_test)\n",
    "svc_train_time, svc_predict_time = measure_time(svc, X_train, X_test, y_train, y_test)\n",
    "knn_train_time, knn_predict_time = measure_time(knn, X_train, X_test, y_train, y_test)\n",
    "\n",
    "print(f\"Naive Bayes - Training Time: {nb_train_time:.4f}s, Prediction Time: {nb_predict_time:.4f}s\")\n",
    "print(f\"Logistic Regression - Training Time: {lr_train_time:.4f}s, Prediction Time: {lr_predict_time:.4f}s\")\n",
    "print(f\"SVC - Training Time: {svc_train_time:.4f}s, Prediction Time: {svc_predict_time:.4f}s\")\n",
    "print(f\"KNN - Training Time: {knn_train_time:.4f}s, Prediction Time: {knn_predict_time:.4f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
